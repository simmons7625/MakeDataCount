{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bb51b9e3-a3cf-459b-b461-290687c6196c",
    "_uuid": "68ddb9cd-125c-47c3-898e-b4133ce5de9c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-01T03:49:06.376918Z",
     "iopub.status.busy": "2025-08-01T03:49:06.376608Z",
     "iopub.status.idle": "2025-08-01T03:49:29.373634Z",
     "shell.execute_reply": "2025-08-01T03:49:29.372761Z",
     "shell.execute_reply.started": "2025-08-01T03:49:06.376898Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 20.083729,
     "end_time": "2025-07-30T12:43:50.835737",
     "exception": false,
     "start_time": "2025-07-30T12:43:30.752008",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! uv pip uninstall --system 'tensorflow'\n",
    "! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'\n",
    "! mkdir -p /tmp/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d83ceec5-2fa5-4e2c-aab0-2b543305470f",
    "_uuid": "0d592448-86ce-43c2-8a60-d64375fe7946",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-01T03:49:29.375596Z",
     "iopub.status.busy": "2025-08-01T03:49:29.375316Z",
     "iopub.status.idle": "2025-08-01T03:49:29.383472Z",
     "shell.execute_reply": "2025-08-01T03:49:29.382574Z",
     "shell.execute_reply.started": "2025-08-01T03:49:29.375571Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018527,
     "end_time": "2025-07-30T12:43:50.864121",
     "exception": false,
     "start_time": "2025-07-30T12:43:50.845594",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/src/helpers.py\n",
    "import logging, os, kagglehub, inspect\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "IS_KAGGLE_ENV = sum(['KAGGLE' in k for k in os.environ]) > 0\n",
    "IS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "COMP_DIR = Path(('/kaggle/input/make-data-count-finding-data-references' if IS_KAGGLE_SUBMISSION else kagglehub.competition_download('make-data-count-finding-data-references')))\n",
    "PDF_DIR = COMP_DIR / ('test' if IS_KAGGLE_SUBMISSION else 'train') / 'PDF'\n",
    "WORKING_DIR = Path(('/kaggle/working/' if IS_KAGGLE_ENV else '.working/'))\n",
    "\n",
    "DOI_LINK = 'https://doi.org/'\n",
    "\n",
    "DEFAULT_LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"DEBUG\").upper() if not IS_KAGGLE_SUBMISSION else \"WARNING\"\n",
    "LOG_FILE_PATH = os.getenv(\"LOG_FILE\", \"logs/project.log\")\n",
    "LOG_DIR = Path(LOG_FILE_PATH).parent\n",
    "\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s  [%(filename)s:%(lineno)d - %(funcName)s()] %(message)s\"\n",
    "LOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def get_logger(name=None):\n",
    "    if name is None:\n",
    "        frame = inspect.currentframe()\n",
    "        if frame is None or frame.f_back is None:\n",
    "            name = \"__main__\"\n",
    "        else:\n",
    "            name = frame.f_back.f_globals.get(\"__name__\", \"__main__\")\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=LOG_DATEFMT)\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        ch.setFormatter(formatter)\n",
    "        fh = logging.FileHandler(LOG_FILE_PATH)\n",
    "        fh.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        logger.addHandler(fh)\n",
    "        logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "def is_doi_link(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.starts_with(DOI_LINK)\n",
    "\n",
    "def string_normalization(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", '').str.replace_all(r\"https?://zenodo\\.org/record/(\\d+)\", r\" 10.5281/zenodo.$1 \")\n",
    "\n",
    "def get_df(parse_dir: str):\n",
    "    records = []\n",
    "    txt_files = list(Path(parse_dir).glob('*.txt'))\n",
    "    for txt_file in txt_files:\n",
    "        id_ = txt_file.stem\n",
    "        with open(txt_file, 'r') as f:\n",
    "            text = f.read()\n",
    "        records.append({'article_id': id_, 'text': text})\n",
    "    return pl.DataFrame(records).with_columns(string_normalization('text').alias('text'))\n",
    "\n",
    "def assume_type(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.with_columns(pl.when(is_doi_link('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise(pl.lit('Secondary')).alias('type'))\n",
    "    )\n",
    "\n",
    "def score(df, gt, on, tag='all'):\n",
    "    hits = gt.join(df, on=on)\n",
    "    tp = hits.height\n",
    "    fp = df.height - tp\n",
    "    fn = gt.height - tp\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    return f\"{tag} - f1: {f1:.4f} [{tp}/{fp}/{fn}]\"\n",
    "\n",
    "def evaluate(df, on=['article_id', 'dataset_id']):\n",
    "    gt = pl.read_csv(COMP_DIR/'train_labels.csv').filter(pl.col('type')!='Missing')\n",
    "    return (\n",
    "        score(df, gt, on),\n",
    "        score(df.filter(is_doi_link('dataset_id')), gt.filter(is_doi_link('dataset_id')), on, 'doi'),\n",
    "        score(df.filter(~is_doi_link('dataset_id')), gt.filter(~is_doi_link('dataset_id')), on, 'acc'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c7060a6d-ef30-4916-b3fc-08b5b4a3d6d9",
    "_uuid": "eb6f0483-05f0-4031-b027-c3cb4129306d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-01T03:49:29.384598Z",
     "iopub.status.busy": "2025-08-01T03:49:29.384265Z",
     "iopub.status.idle": "2025-08-01T03:49:31.041735Z",
     "shell.execute_reply": "2025-08-01T03:49:31.040864Z",
     "shell.execute_reply.started": "2025-08-01T03:49:29.38457Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014728,
     "end_time": "2025-07-30T12:43:50.888086",
     "exception": false,
     "start_time": "2025-07-30T12:43:50.873358",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/src/parse.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pymupdf\n",
    "from helpers import get_logger, PDF_DIR\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def pdf_to_txt(output_dir: Path):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_files = list(PDF_DIR.glob(\"*.pdf\")) + list(PDF_DIR.glob(\"*.PDF\"))\n",
    "    existing_txt_files = {f.stem for f in output_dir.glob(\"*.txt\")}\n",
    "    for pdf_file in pdf_files:\n",
    "        txt_file = output_dir / f\"{pdf_file.stem}.txt\"\n",
    "        if pdf_file.stem in existing_txt_files:\n",
    "            continue\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with pymupdf.open(pdf_file) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            txt_file.write_text(text, encoding='utf-8')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('output_dir', type=Path, help='Directory to save text files')\n",
    "    args = parser.parse_args()\n",
    "    pdf_to_txt(args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "573a2751-a0a6-4d64-8786-da86453a2fda",
    "_uuid": "165f2fe5-dd88-475a-b589-cf17d1ca97d7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-01T03:49:31.043846Z",
     "iopub.status.busy": "2025-08-01T03:49:31.043614Z",
     "iopub.status.idle": "2025-08-01T03:49:31.056043Z",
     "shell.execute_reply": "2025-08-01T03:49:31.055301Z",
     "shell.execute_reply.started": "2025-08-01T03:49:31.043828Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016232,
     "end_time": "2025-07-30T12:43:50.913072",
     "exception": false,
     "start_time": "2025-07-30T12:43:50.89684",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/src/check_parse.py\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from helpers import *\n",
    "\n",
    "l=get_logger()\n",
    "\n",
    "def gt_dataset_id_normalization(name:str) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when(is_doi_link(name))\n",
    "        .then(pl.col(name).str.split(DOI_LINK).list.last())\n",
    "        .otherwise(name)\n",
    "        .str.to_lowercase()\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    if IS_KAGGLE_SUBMISSION:\n",
    "        l.debug('skipping check_parse for submission')\n",
    "        return\n",
    "    df = (\n",
    "        get_df('/tmp/train_parse')\n",
    "        .with_columns(pl.col('text').str.replace_all('\\s+', '').str.to_lowercase().alias('text'))\n",
    "    )\n",
    "\n",
    "    gt = (\n",
    "        pl.read_csv(COMP_DIR/'train_labels.csv')\n",
    "        .filter(pl.col('article_id').is_in(df['article_id']))\n",
    "        .filter(pl.col('type')!='Missing')\n",
    "        .with_columns(gt_dataset_id_normalization('dataset_id').alias('norm_id'))\n",
    "    )\n",
    "\n",
    "    l.info(f\"pymupdf misses: {gt.join(df, on='article_id').with_columns(hit=pl.col('text').str.contains(pl.col('norm_id'), literal=True)).filter(~pl.col('hit')).height} dataset_ids\")\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "af0b3ba0-38a0-49e6-9d58-2c7ba1618484",
    "_uuid": "8eea8c4d-a332-4622-817c-b4949f6d8cac",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-01T03:49:31.057175Z",
     "iopub.status.busy": "2025-08-01T03:49:31.056944Z",
     "iopub.status.idle": "2025-08-01T03:49:31.070338Z",
     "shell.execute_reply": "2025-08-01T03:49:31.069697Z",
     "shell.execute_reply.started": "2025-08-01T03:49:31.057148Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019717,
     "end_time": "2025-07-30T12:43:50.939863",
     "exception": false,
     "start_time": "2025-07-30T12:43:50.920146",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/src/getid.py\n",
    "import re\n",
    "import polars as pl\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "COMPILED_PATTERNS = {\n",
    "    'ref_header_patterns': [re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS|ACKNOWLEDGEMENTS)\\b[:\\s]*', re.IGNORECASE)],    \n",
    "    'citation_pattern': re.compile(r'^\\s*(\\[\\d+\\]|\\(\\d+\\)|\\d+\\.|\\d+\\)|\\d+(?=\\s|$))\\s*'),\n",
    "    'first_citation_patterns': [\n",
    "        re.compile(r'^\\s*\\[1\\]\\s*'),\n",
    "        re.compile(r'^\\s*\\(1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1\\.\\s*'),\n",
    "        re.compile(r'^\\s*1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1(?=\\s|$)'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n",
    "    last_match_idx = None\n",
    "    for pattern in header_patterns:\n",
    "        matches = list(pattern.finditer(text))\n",
    "        if matches:\n",
    "            last_match_idx = matches[-1].start()\n",
    "    return last_match_idx\n",
    "\n",
    "def find_last_first_citation(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_match_line = None\n",
    "    for line_num, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n",
    "            if pattern.match(line):\n",
    "                next_lines = lines[line_num:line_num+3]\n",
    "                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n",
    "                    last_match_line = line_num\n",
    "                break\n",
    "    return last_match_line\n",
    "\n",
    "def find_reference_start(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_first_citation = find_last_first_citation(text)\n",
    "    if last_first_citation is not None:\n",
    "        return last_first_citation\n",
    "    start_search_idx = int(len(lines) * 0.5)\n",
    "    for i in range(start_search_idx, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if COMPILED_PATTERNS['citation_pattern'].match(line):\n",
    "            next_lines = lines[i:i+3]\n",
    "            if sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip())) >= 2:\n",
    "                for j in range(i, max(-1, i-10), -1):\n",
    "                    if not COMPILED_PATTERNS['citation_pattern'].match(lines[j].strip()):\n",
    "                        return j + 1\n",
    "                return max(0, i-10)\n",
    "    return None\n",
    "\n",
    "def split_text_and_references(text: str) -> Tuple[str, str]:\n",
    "    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n",
    "    if header_idx is not None:\n",
    "        header_idx2 = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "        if header_idx2 is not None:\n",
    "            header_idx3 = find_last_reference_header(text[:header_idx2].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "            if header_idx3 is not None:\n",
    "                return text[:header_idx3].strip(), text[header_idx3:].strip()\n",
    "            return text[:header_idx2].strip(), text[header_idx2:].strip()\n",
    "        return text[:header_idx].strip(), text[header_idx:].strip()\n",
    "    ref_start_line = find_reference_start(text)\n",
    "    if ref_start_line is not None:\n",
    "        lines = text.splitlines()\n",
    "        body = '\\n'.join(lines[:ref_start_line])\n",
    "        refs = '\\n'.join(lines[ref_start_line:])\n",
    "        return body.strip(), refs.strip()\n",
    "    return text.strip(), ''\n",
    "\n",
    "def get_splits(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    bodies, refs = [], []\n",
    "    for raw_text in df['text']:\n",
    "        main, ref = split_text_and_references(raw_text)\n",
    "        bodies.append(main)\n",
    "        refs.append(ref)\n",
    "    return df.with_columns(pl.Series('body', bodies), pl.Series('ref', refs))\n",
    "\n",
    "def tidy_extraction(df) -> pl.DataFrame:\n",
    "    bad_ids = [f'{DOI_LINK}{e}' for e in ['10.5061/dryad', '10.5281/zenodo', '10.6073/pasta']]\n",
    "\n",
    "    doi_df = (\n",
    "        df.with_columns(pl.col('body').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match'))\n",
    "          .explode('match')\n",
    "          .drop_nulls('match')\n",
    "          .with_columns(\n",
    "              pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                             .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                             .str.to_lowercase()\n",
    "                             .alias('dataset_id')\n",
    "          )\n",
    "          .group_by('article_id', 'dataset_id')\n",
    "          .agg('match')\n",
    "          .with_columns((DOI_LINK + pl.col('dataset_id')).alias('dataset_id'))\n",
    "    )\n",
    "\n",
    "    REGEX_IDS = (\n",
    "        r\"(?i)\\b(?:\"\n",
    "        r\"CHEMBL\\d+|\"\n",
    "        r\"E-GEOD-\\d+|E-PROT-\\d+|E-MTAB-\\d+|E-MEXP-\\d+|EMPIAR-\\d+|\"\n",
    "        r\"ENSBTAG\\d+|ENSOARG\\d+|\"\n",
    "        r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n",
    "        r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|BX\\d{6}|KX\\d{6}|K0\\d{4}|CAB\\d{6}|\"\n",
    "        r\"NC_\\d{6}\\.\\d{1}|NM_\\d{9}|\"\n",
    "        r\"PRJNA\\d+|PRJDB\\d+|PXD\\d+|SAMN\\d+|\"\n",
    "        r\"GSE\\d+|GSM\\d+|GPL\\d+|\"\n",
    "        r\"PDB\\s?[1-9][A-Z0-9]{3}|HMDB\\d+|\"\n",
    "        r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+|\"\n",
    "        r\"(?:SR[PX]|STH|ERR|DRR|DRX|DRP|ERP|ERX)\\d+|\"\n",
    "        r\"CVCL_[A-Z0-9]{4}\"\n",
    "        r\")\"\n",
    "    )\n",
    "    \n",
    "    acc_df = (\n",
    "        df.with_columns(\n",
    "            pl.col('text').str.extract_all(REGEX_IDS).alias('match')\n",
    "        )\n",
    "        .explode('match')\n",
    "        .drop_nulls('match')\n",
    "        .with_columns(\n",
    "            pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                           .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                           .str.replace(r'(?i)^PDB', '')\n",
    "                           .alias('dataset_id')\n",
    "        )\n",
    "        .group_by('article_id', 'dataset_id')\n",
    "        .agg('match')\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('dryad.'))\n",
    "              .then(f'{DOI_LINK}10.5061/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('pasta/'))\n",
    "              .then(f'{DOI_LINK}10.6073/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = pl.concat([doi_df, acc_df])\n",
    "\n",
    "    df = (\n",
    "        df.unique(['article_id', 'dataset_id'])  # CHANGED\n",
    "          .filter(~pl.col('article_id').str.replace('_','/').str.contains(pl.col('dataset_id').str.split(DOI_LINK).list.last().str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains(pl.col('article_id').str.replace('_','/').str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains('figshare', literal=True))\n",
    "          .filter(~pl.col('dataset_id').is_in(bad_ids))\n",
    "          .filter(\n",
    "              pl.when(is_doi_link('dataset_id') &\n",
    "                      (pl.col('dataset_id').str.split('/').list.last().str.len_chars() < 5))\n",
    "               .then(False)\n",
    "               .otherwise(True)\n",
    "          )\n",
    "          .with_columns(pl.col('match').list.unique())\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 100) -> str:\n",
    "    idx = text.find(substring)\n",
    "    if idx == -1:\n",
    "        raise ValueError\n",
    "    start = max(idx - window, 0)\n",
    "    end = min(idx + len(substring) + window, len(text))\n",
    "    return text[start:end]\n",
    "\n",
    "def get_window_df(text_df, ids_df):\n",
    "    df = ids_df.join(text_df, on='article_id')\n",
    "    windows = []\n",
    "    for text, match_ids in df.select('text', 'match').rows():\n",
    "        windows.append(get_context_window(text, match_ids[0]))\n",
    "    return df.with_columns(pl.Series('window', windows)).select('article_id', 'dataset_id', 'window')\n",
    "\n",
    "def main():\n",
    "    text_df = get_df('/tmp/train_parse')\n",
    "    df = get_splits(text_df)\n",
    "    df = tidy_extraction(df)\n",
    "    df = get_window_df(text_df, df)\n",
    "    df.write_parquet('/tmp/extracted.parquet')\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r)\n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "378df5d8-fad7-4982-9948-ab64de94454d",
    "_uuid": "bdaa185a-0977-417c-b783-32b1b556214a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-01T03:49:31.071381Z",
     "iopub.status.busy": "2025-08-01T03:49:31.071193Z",
     "iopub.status.idle": "2025-08-01T03:49:31.085361Z",
     "shell.execute_reply": "2025-08-01T03:49:31.084469Z",
     "shell.execute_reply.started": "2025-08-01T03:49:31.071365Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019658,
     "end_time": "2025-07-30T12:43:50.971551",
     "exception": false,
     "start_time": "2025-07-30T12:43:50.951893",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/src/llm_validate.py\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n",
    "1. Priority Rules (highest → lowest)\n",
    "1.1 Always classify as A (Data) if:\n",
    "DOI prefix matches a known data repository:\n",
    "\n",
    "Dryad: 10.5061\n",
    "\n",
    "Zenodo: 10.5281\n",
    "\n",
    "Figshare: 10.6084\n",
    "\n",
    "Mendeley Data: 10.24433/, 10.17632\n",
    "\n",
    "Dataverse: 10.7910/DVN\n",
    "\n",
    "OpenNeuro: 10.18112/openneuro.\n",
    "\n",
    "PANGAEA: 10.1594/PANGAEA.\n",
    "\n",
    "Neotoma Paleoecology: 10.21233\n",
    "\n",
    "ICPSR: 10.3886\n",
    "\n",
    "NOAA NCEI: 10.7289\n",
    "\n",
    "UK Data Service: 10.5255\n",
    "\n",
    "EMPIAR: 10.6019\n",
    "\n",
    "Non-DOI dataset accession prefixes:\n",
    "\n",
    "NCBI SRA / ENA: SRP, SRA, ERP, ERX\n",
    "\n",
    "BioProject: PRJNA, PRJEB, PRJDB\n",
    "\n",
    "ProteomeXchange / PRIDE: PXD\n",
    "\n",
    "ArrayExpress / EMBL-EBI: E-MTAB, E-\n",
    "\n",
    "MetaboLights: MTBLS\n",
    "\n",
    "GEO Series: GSE\n",
    "\n",
    "GenBank: MN, NC_, CP, MT (context needed)\n",
    "\n",
    "EMDB: EMD-\n",
    "\n",
    "EMPIAR: EMPIAR-\n",
    "\n",
    "1.2 Context keywords trigger A (Data)\n",
    "Even if the prefix is not listed above, classify as A if the context clearly indicates dataset storage.\n",
    "Keywords (case-insensitive, include plural forms):\n",
    "\n",
    "dataset, data set\n",
    "\n",
    "data repository, data archive, data portal\n",
    "\n",
    "deposited in, uploaded to, archived at\n",
    "\n",
    "available at, stored on, hosted by\n",
    "\n",
    "accessible via, retrieved from, provided by\n",
    "\n",
    "supplementary dataset, supporting dataset\n",
    "\n",
    "experimental data, raw data\n",
    "\n",
    "public repository\n",
    "\n",
    "2. Classify as B (Literature) if:\n",
    "DOI prefix belongs to a publisher (e.g., 10.1038, 10.1007, 10.1126, 10.1016, 10.1101, 10.1021, 10.1145, 10.1177, 10.1093, 10.1080, 10.1111, etc.).\n",
    "\n",
    "Context indicates a journal article, book, conference paper, preprint, protocol, or method paper, without any repository/data storage signal.\n",
    "\n",
    "Mentions only “supplementary material” or “supplementary information” without a repository.\n",
    "\n",
    "3. Ambiguous cases\n",
    "No repository prefix and no clear context → default to B.\n",
    "\n",
    "Rare accession formats → rely on context keywords.\n",
    "\n",
    "4. Output\n",
    "Only output:\n",
    "\n",
    "A → data repository / dataset\n",
    "\n",
    "B → literature / non-data resource\n",
    "\n",
    "Few-shot examples\n",
    "\n",
    "“Raw images are stored on Figshare (DOI 10.6084/m9.figshare.1234567).” → A\n",
    "\n",
    "“Sequence reads available under BioProject accession PRJNA765432.” → A\n",
    "\n",
    "“As described in Nature Methods (DOI 10.1038/s41592-020-0793-2).” → B\n",
    "\n",
    "“See Supplementary Data at Zenodo (10.5281/zenodo.987654).” → A\n",
    "\n",
    "“Method details published in J. Proteome Res. DOI: 10.1021/acs.jproteome.0c00845.” → B\n",
    "\n",
    "“Data uploaded to Dryad (10.5061/dryad.x1y2z3).” → A\n",
    "\n",
    "“Referenced paper: DOI 10.1101/2020.01.01.123456 (bioRxiv preprint).” → B\n",
    "\n",
    "“Metabolomics data in MetaboLights MTBLS1234.” → A\n",
    "\n",
    "“The MRI scans are deposited at OpenNeuro (DOI 10.18112/openneuro.ds000001.v1.0.0).” → A\n",
    "\n",
    "“Protein structure described in Science (DOI 10.1126/science.abc1234).” → B\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_df():\n",
    "    df = pl.read_parquet('/tmp/extracted.parquet')\n",
    "    df.filter(~is_doi_link('dataset_id')).select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n",
    "    return df.filter(is_doi_link('dataset_id'))\n",
    "\n",
    "def build_prompt(tokenizer, df):\n",
    "    prompts = []\n",
    "    for doi, text in df.select('dataset_id', 'window').rows():\n",
    "        messages = [{'role':'system','content': SYS_PROMPT_CLASSIFY_DOI}, {'role':'user', 'content': text}]\n",
    "        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False))\n",
    "    return df.with_columns(pl.Series('prompt', prompts))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    llm = vllm.LLM(model_path, quantization='awq', tensor_parallel_size=2, gpu_memory_utilization=0.9, trust_remote_code=True, dtype=\"half\", enforce_eager=True, max_model_len=2048, disable_log_stats=True, disable_custom_all_reduce=True, enable_prefix_caching=True, task='generate')\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    df = build_df()\n",
    "    df = build_prompt(tokenizer, df)\n",
    "    prompts = df['prompt'].to_list()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A': True, 'B': False}\n",
    "    choices = [types[c] for c in choices]\n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "27022685-0893-460a-8ea4-5c62fd964ae8",
    "_uuid": "24048569-bb55-4720-8252-1bfdeca91805",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-01T03:49:31.086528Z",
     "iopub.status.busy": "2025-08-01T03:49:31.086302Z",
     "iopub.status.idle": "2025-08-01T03:49:31.097894Z",
     "shell.execute_reply": "2025-08-01T03:49:31.097246Z",
     "shell.execute_reply.started": "2025-08-01T03:49:31.086488Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018508,
     "end_time": "2025-07-30T12:43:51.002131",
     "exception": false,
     "start_time": "2025-07-30T12:43:50.983623",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/src/post_filter.py\n",
    "import polars as pl\n",
    "from helpers import *\n",
    "\n",
    "\"\"\"\n",
    "Fourth essence: Post-filter to cut FP DOIs that look like literature.\n",
    "- Read /kaggle/working/submission.csv (output of llm_validate.py)\n",
    "- Join with /tmp/extracted.parquet to get context window\n",
    "- Drop DOI rows that (1) start with typical publisher prefixes AND (2) have no data-ish words nearby\n",
    "- Keep accessions untouched\n",
    "\"\"\"\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "PAPER_PREFIXES = [\n",
    "    \"10.5061\",\"10.5281\",\"10.17632\",\"10.1594\",\"10.15468\",\"10.17882\",\"10.7937\",\"10.7910\",\"10.6073\",\n",
    "    \"10.3886\",\"10.3334\",\"10.4121\",\"10.5066\",\"10.5067\",\"10.18150\",\"10.25377\",\"10.25387\",\"10.23642\",\"10.24381\",\"10.22033\"\n",
    "]\n",
    "\n",
    "CONTEXT_RE = r\"(?i)\\b(data(?:set)?|repository|archive|deposited|available|supplementary|raw(?:\\s+data)?|uploaded|hosted|stored|accession)\\b\"\n",
    "\n",
    "def is_paper_prefix(col: str = \"dataset_id\") -> pl.Expr:\n",
    "    expr = pl.lit(False)\n",
    "    for p in PAPER_PREFIXES:\n",
    "        expr = expr | pl.col(col).str.starts_with(f\"{DOI_LINK}{p}\")\n",
    "    return expr\n",
    "\n",
    "def main():\n",
    "    sub = pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "    # Normalize columns: drop row_id if present so concat widths match\n",
    "    if \"row_id\" in sub.columns:\n",
    "        sub = sub.drop(\"row_id\")\n",
    "\n",
    "    # Context windows\n",
    "    win = pl.read_parquet(\"/tmp/extracted.parquet\").select(\"article_id\", \"dataset_id\", \"window\")\n",
    "\n",
    "    # DOI & ACC split\n",
    "    doi_rows = sub.filter(is_doi_link(\"dataset_id\")).join(win, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "    acc_rows = sub.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "    keep_mask = (\n",
    "        (~is_paper_prefix(\"dataset_id\"))  # not a known paper prefix\n",
    "        | doi_rows[\"window\"].fill_null(\"\").str.contains(CONTEXT_RE)\n",
    "    )\n",
    "\n",
    "    kept_doi = doi_rows.filter(keep_mask).select(\"article_id\", \"dataset_id\", \"type\")\n",
    "    final = pl.concat([kept_doi, acc_rows.select(\"article_id\", \"dataset_id\", \"type\")])\n",
    "\n",
    "    # Re-eval & save\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        for r in evaluate(final): l.info(r)\n",
    "        for r in evaluate(final, on=[\"article_id\", \"dataset_id\", \"type\"]): l.info(r)\n",
    "\n",
    "    final.with_row_index(\"row_id\").write_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "819a5328-e369-4aec-aa6d-a1090924dc14",
    "_kg_hide-output": true,
    "_uuid": "d453e324-769e-4364-883e-36dc9f68cab8",
    "collapsed": false,
    "execution": {
     "execution_failed": "2025-08-01T03:54:18.903Z",
     "iopub.execute_input": "2025-08-01T03:49:31.098966Z",
     "iopub.status.busy": "2025-08-01T03:49:31.098697Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 448.641642,
     "end_time": "2025-07-30T12:51:19.65281",
     "exception": false,
     "start_time": "2025-07-30T12:43:51.011168",
     "status": "completed"
    },
    "scrolled": true,
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /tmp\n",
    "!LOG_LEVEL=INFO python src/parse.py /tmp/train_parse\n",
    "! python src/check_parse.py\n",
    "! python src/getid.py\n",
    "! python src/llm_validate.py\n",
    "! python src/post_filter.py\n",
    "! grep \"f1:\" /tmp/logs/project.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "104fd444-60c0-437d-9d83-8f169b59ecab",
    "_uuid": "3f69cbb6-58cf-4c41-b511-d7bbcad754d3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009673,
     "end_time": "2025-07-30T12:51:19.672901",
     "exception": false,
     "start_time": "2025-07-30T12:51:19.663228",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13015230,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "sourceId": 248118764,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141565,
     "sourceId": 166368,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 474.866415,
   "end_time": "2025-07-30T12:51:20.001095",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-30T12:43:25.13468",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
