{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文ラベルごとの頻出単語分析\n",
    "\n",
    "論文のラベル（Primary, Secondary, Missing）ごとに頻出単語を分析し、特徴的な単語を抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport re\nfrom collections import Counter, defaultdict\nimport fitz  # PyMuPDF\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Download required NLTK data\nnltk.download('stopwords', quiet=True)\nnltk.download('punkt', quiet=True)\nnltk.download('wordnet', quiet=True)\n\n# Set up plotting\nplt.style.use('default')\nsns.set_palette('husl')\nplt.rcParams['figure.figsize'] = (12, 8)\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training labels\ntrain_labels = pd.read_csv('../data/train_labels.csv')\nprint(f\"Training labels shape: {train_labels.shape}\")\nprint(\"\\nLabel distribution:\")\nprint(train_labels['type'].value_counts())\nprint(f\"\\nPercentages:\")\nprint((train_labels['type'].value_counts() / len(train_labels) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up data paths\ntrain_dir = Path('../data/train')\npdf_dir = train_dir / 'PDF'\n\nprint(f\"PDF files: {len(list(pdf_dir.glob('*.pdf'))) if pdf_dir.exists() else 0}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. テキスト抽出関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_text_from_pdf(pdf_path):\n    \"\"\"PDFファイルからテキストを抽出\"\"\"\n    try:\n        # PyMuPDFでPDFを開く\n        doc = fitz.open(pdf_path)\n        text_parts = []\n        \n        # 各ページからテキストを抽出\n        for page_num in range(len(doc)):\n            page = doc.load_page(page_num)\n            text = page.get_text()\n            if text.strip():  # 空でない場合のみ追加\n                text_parts.append(text)\n        \n        doc.close()\n        return ' '.join(text_parts)\n    \n    except Exception as e:\n        print(f\"Error processing {pdf_path}: {e}\")\n        return \"\"\n\ndef preprocess_text(text):\n    \"\"\"テキストの前処理\"\"\"\n    if not text:\n        return []\n    \n    # 小文字化\n    text = text.lower()\n    \n    # 数字、特殊文字を除去\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # トークン化\n    tokens = word_tokenize(text)\n    \n    # ストップワード除去\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n    \n    # レンマ化\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n    \n    return tokens"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 論文からテキストを抽出してラベル別に分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ラベル別のテキストを格納する辞書\nlabel_texts = {\n    'Primary': [],\n    'Secondary': [],\n    'Missing': []\n}\n\n# 各論文のテキストを抽出\nprocessed_count = 0\n\nfor _, row in train_labels.iterrows():\n    article_id = row['article_id']\n    label = row['type']\n    \n    # PDFファイルのパスを構築\n    pdf_path = pdf_dir / f\"{article_id}.pdf\"\n    \n    if pdf_path.exists():\n        text = extract_text_from_pdf(pdf_path)\n        if text:\n            label_texts[label].append(text)\n            processed_count += 1\n    \n    # 進捗表示\n    if processed_count % 50 == 0:\n        print(f\"Processed {processed_count} articles...\")\n\nprint(f\"\\nTotal processed articles: {processed_count}\")\nprint(\"\\nTexts per label:\")\nfor label, texts in label_texts.items():\n    print(f\"  {label}: {len(texts)} articles\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ラベル別の単語頻度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベル別の単語カウント\nlabel_word_counts = {}\nlabel_vocabularies = {}\n\nfor label, texts in label_texts.items():\n",
    "    print(f\"\\nProcessing {label} articles...\")\n",
    "    \n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = preprocess_text(text)\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # 単語頻度を計算\n",
    "    word_counts = Counter(all_words)\n",
    "    label_word_counts[label] = word_counts\n",
    "    label_vocabularies[label] = set(all_words)\n",
    "    \n",
    "    print(f\"  Total words: {len(all_words):,}\")\n",
    "    print(f\"  Unique words: {len(word_counts):,}\")\n",
    "    print(f\"  Top 10 words: {word_counts.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 頻出単語の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ラベルの頻出単語を棒グラフで可視化\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))\n\nfor i, (label, word_counts) in enumerate(label_word_counts.items()):\n",
    "    if len(word_counts) > 0:\n",
    "        top_words = word_counts.most_common(15)\n",
    "        words, counts = zip(*top_words)\n",
    "        \n",
    "        axes[i].barh(range(len(words)), counts)\n",
    "        axes[i].set_yticks(range(len(words)))\n",
    "        axes[i].set_yticklabels(words)\n",
    "        axes[i].set_xlabel('頻度')\n",
    "        axes[i].set_title(f'{label} ラベル - 頻出単語 Top 15')\n",
    "        axes[i].invert_yaxis()\n",
    "\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ワードクラウドの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ラベルのワードクラウドを作成\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))\n\nfor i, (label, word_counts) in enumerate(label_word_counts.items()):\n",
    "    if len(word_counts) > 0:\n",
    "        # ワードクラウドの作成\n",
    "        wordcloud = WordCloud(width=400, height=300, background_color='white').generate_from_frequencies(word_counts)\n",
    "        \n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].set_title(f'{label} ラベル - ワードクラウド')\n",
    "        axes[i].axis('off')\n",
    "\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ラベル間の特徴的な単語の分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDFスコアを計算して特徴的な単語を抽出\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# 各ラベルの全テキストを結合\nlabel_documents = {}\nfor label, texts in label_texts.items():\n    if texts:\n        # 前処理済みのテキストを結合\n        processed_texts = [' '.join(preprocess_text(text)) for text in texts]\n        label_documents[label] = ' '.join(processed_texts)\n\n# TF-IDFベクトライザーを作成\nif label_documents:\n    documents = list(label_documents.values())\n    labels = list(label_documents.keys())\n    \n    vectorizer = TfidfVectorizer(max_features=1000, min_df=2)\n    tfidf_matrix = vectorizer.fit_transform(documents)\n    \n    # 特徴語の抽出\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # 各ラベルの特徴的な単語を抽出\n    for i, label in enumerate(labels):\n        # TF-IDFスコアを取得\n        tfidf_scores = tfidf_matrix[i].toarray()[0]\n        \n        # スコアでソート\n        word_scores = list(zip(feature_names, tfidf_scores))\n        word_scores.sort(key=lambda x: x[1], reverse=True)\n        \n        print(f\"\\n=== {label} ラベルの特徴的な単語 (TF-IDF) ===\")\n        for word, score in word_scores[:20]:\n            print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ラベル間の語彙の重複分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語彙の重複を分析\nif len(label_vocabularies) >= 2:\n",
    "    print(\"=== 語彙の重複分析 ===\")\n",
    "    \n",
    "    # 各ラベルの語彙サイズ\n",
    "    for label, vocab in label_vocabularies.items():\n",
    "        print(f\"{label}: {len(vocab):,} unique words\")\n",
    "    \n",
    "    # ペアワイズの重複を計算\n",
    "    labels = list(label_vocabularies.keys())\n",
    "    print(\"\\n=== ラベル間の語彙重複 ===\")\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i+1, len(labels)):\n",
    "            label1, label2 = labels[i], labels[j]\n",
    "            vocab1, vocab2 = label_vocabularies[label1], label_vocabularies[label2]\n",
    "            \n",
    "            intersection = vocab1 & vocab2\n",
    "            union = vocab1 | vocab2\n",
    "            \n",
    "            jaccard = len(intersection) / len(union) if union else 0\n",
    "            \n",
    "            print(f\"{label1} vs {label2}:\")\n",
    "            print(f\"  共通語彙: {len(intersection):,}\")\n",
    "            print(f\"  Jaccard係数: {jaccard:.4f}\")\n",
    "            print(f\"  重複率 ({label1}): {len(intersection)/len(vocab1)*100:.2f}%\")\n",
    "            print(f\"  重複率 ({label2}): {len(intersection)/len(vocab2)*100:.2f}%\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ラベル別の語彙統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語彙統計の可視化\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# 1. 語彙サイズの比較\nlabels = list(label_vocabularies.keys())\nvocab_sizes = [len(vocab) for vocab in label_vocabularies.values()]\n\naxes[0, 0].bar(labels, vocab_sizes)\naxes[0, 0].set_title('ラベル別語彙サイズ')\naxes[0, 0].set_ylabel('ユニーク単語数')\n\n# 2. 単語頻度の分布\nfor i, (label, word_counts) in enumerate(label_word_counts.items()):\n    if len(word_counts) > 0:\n        frequencies = list(word_counts.values())\n        axes[0, 1].hist(frequencies, bins=50, alpha=0.7, label=label)\n\naxes[0, 1].set_title('単語頻度の分布')\naxes[0, 1].set_xlabel('頻度')\naxes[0, 1].set_ylabel('単語数')\naxes[0, 1].set_yscale('log')\naxes[0, 1].legend()\n\n# 3. 高頻度語の累積分布\nfor i, (label, word_counts) in enumerate(label_word_counts.items()):\n    if len(word_counts) > 0:\n        frequencies = sorted(word_counts.values(), reverse=True)\n        cumulative = np.cumsum(frequencies)\n        axes[1, 0].plot(cumulative/cumulative[-1], label=label)\n\naxes[1, 0].set_title('単語頻度の累積分布')\naxes[1, 0].set_xlabel('単語ランク（正規化）')\naxes[1, 0].set_ylabel('累積頻度（正規化）')\naxes[1, 0].legend()\n\n# 4. 語彙の重複関係（ベン図風）\nif len(label_vocabularies) >= 2:\n    overlap_matrix = np.zeros((len(labels), len(labels)))\n    \n    for i, label1 in enumerate(labels):\n        for j, label2 in enumerate(labels):\n            if i != j:\n                vocab1, vocab2 = label_vocabularies[label1], label_vocabularies[label2]\n                overlap = len(vocab1 & vocab2) / len(vocab1 | vocab2)\n                overlap_matrix[i, j] = overlap\n    \n    im = axes[1, 1].imshow(overlap_matrix, cmap='Blues')\n    axes[1, 1].set_xticks(range(len(labels)))\n    axes[1, 1].set_yticks(range(len(labels)))\n    axes[1, 1].set_xticklabels(labels)\n    axes[1, 1].set_yticklabels(labels)\n    axes[1, 1].set_title('語彙重複率（Jaccard係数）')\n    \n    # 数値を表示\n    for i in range(len(labels)):\n        for j in range(len(labels)):\n            if i != j:\n                axes[1, 1].text(j, i, f'{overlap_matrix[i, j]:.3f}', \n                               ha='center', va='center')\n    \n    plt.colorbar(im, ax=axes[1, 1])\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 結果の要約と考察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 分析結果の要約 ===\")\n\nprint(\"\\n1. 処理した論文数:\")\nfor label, texts in label_texts.items():\n    print(f\"   {label}: {len(texts)} 論文\")\n\nprint(\"\\n2. 語彙統計:\")\nfor label, vocab in label_vocabularies.items():\n    total_words = sum(label_word_counts[label].values())\n    unique_words = len(vocab)\n    print(f\"   {label}:\")\n    print(f\"     総単語数: {total_words:,}\")\n    print(f\"     ユニーク単語数: {unique_words:,}\")\n    print(f\"     語彙多様性: {unique_words/total_words:.4f}\")\n\nprint(\"\\n3. 各ラベルの特徴的な単語（頻度上位5位）:\")\nfor label, word_counts in label_word_counts.items():\n    if len(word_counts) > 0:\n        top_words = word_counts.most_common(5)\n        print(f\"   {label}: {[word for word, count in top_words]}\")\n\nprint(\"\\n4. 考察:\")\nprint(\"   - 各ラベルの語彙特性が異なることが確認できる\")\nprint(\"   - Primary/Secondaryラベルは研究内容に関する専門用語が多い\")\nprint(\"   - Missingラベルは参考文献の記述に特徴がある可能性\")\nprint(\"   - TF-IDFスコアにより各ラベルの特徴的な単語を特定できた\")\nprint(\"   - 語彙の重複度合いからラベル間の関係性が見える\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}