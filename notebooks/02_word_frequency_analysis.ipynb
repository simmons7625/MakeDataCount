{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文ラベルごとの頻出単語分析\n",
    "\n",
    "論文のラベル（Primary, Secondary, Missing）ごとに頻出単語を分析し、特徴的な単語を抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "japanize_matplotlib.japanize()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (1028, 3)\n",
      "\n",
      "Label distribution:\n",
      "type\n",
      "Secondary    449\n",
      "Missing      309\n",
      "Primary      270\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "type\n",
      "Secondary    43.68\n",
      "Missing      30.06\n",
      "Primary      26.26\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load training labels\n",
    "train_labels = pd.read_csv('../dataset/train_labels.csv')\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(train_labels['type'].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print((train_labels['type'].value_counts() / len(train_labels) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files: 524\n"
     ]
    }
   ],
   "source": [
    "# Set up data paths\n",
    "train_dir = Path('../dataset/train')\n",
    "pdf_dir = train_dir / 'PDF'\n",
    "\n",
    "print(f\"PDF files: {len(list(pdf_dir.glob('*.pdf'))) if pdf_dir.exists() else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト抽出関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"PDFファイルからテキストを抽出\"\"\"\n",
    "    try:\n",
    "        # PyMuPDFでPDFを開く（アノテーションエラーを抑制）\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text_parts = []\n",
    "        \n",
    "        # 各ページからテキストを抽出\n",
    "        for page_num in range(len(doc)):\n",
    "            try:\n",
    "                page = doc.load_page(page_num)\n",
    "                # アノテーションエラーを回避するため、シンプルなテキスト抽出を使用\n",
    "                text = page.get_text(\"text\", flags=fitz.TEXTFLAGS_TEXT)\n",
    "                if text.strip():  # 空でない場合のみ追加\n",
    "                    text_parts.append(text)\n",
    "            except Exception as page_error:\n",
    "                # ページレベルのエラーをスキップして次のページを処理\n",
    "                print(f\"Warning: Error processing page {page_num} in {pdf_path}: {page_error}\")\n",
    "                continue\n",
    "        \n",
    "        doc.close()\n",
    "        return ' '.join(text_parts)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"テキストの前処理\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # 小文字化\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 数字、特殊文字を除去\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # トークン化\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # ストップワード除去\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # レンマ化\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 論文からテキストを抽出してラベル別に分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "\n",
      "Total processed articles: 1028\n",
      "\n",
      "Texts per label:\n",
      "  Primary: 270 articles\n",
      "  Secondary: 449 articles\n",
      "  Missing: 309 articles\n"
     ]
    }
   ],
   "source": [
    "# ラベル別のテキストを格納する辞書\n",
    "label_texts = {\n",
    "    'Primary': [],\n",
    "    'Secondary': [],\n",
    "    'Missing': []\n",
    "}\n",
    "\n",
    "# 各論文のテキストを抽出\n",
    "processed_count = 0\n",
    "\n",
    "for _, row in train_labels.iterrows():\n",
    "    article_id = row['article_id']\n",
    "    label = row['type']\n",
    "    \n",
    "    # PDFファイルのパスを構築\n",
    "    pdf_path = pdf_dir / f\"{article_id}.pdf\"\n",
    "    \n",
    "    if pdf_path.exists():\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            label_texts[label].append(text)\n",
    "            processed_count += 1\n",
    "\n",
    "print(f\"\\nTotal processed articles: {processed_count}\")\n",
    "print(\"\\nTexts per label:\")\n",
    "for label, texts in label_texts.items():\n",
    "    print(f\"  {label}: {len(texts)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベル間の特徴的な単語の分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDFスコアを計算して特徴的な単語を抽出\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 各ラベルの全テキストを結合\n",
    "label_documents = {}\n",
    "for label, texts in label_texts.items():\n",
    "    if texts:\n",
    "        # 前処理済みのテキストを結合\n",
    "        processed_texts = [' '.join(preprocess_text(text)) for text in texts]\n",
    "        label_documents[label] = ' '.join(processed_texts)\n",
    "\n",
    "# TF-IDFベクトライザーを作成\n",
    "if label_documents:\n",
    "    documents = list(label_documents.values())\n",
    "    labels = list(label_documents.keys())\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=1000, min_df=2)\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # 特徴語の抽出\n",
    "    feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Primary ラベルの特徴的な単語 (TF-IDF) ===\n",
      "  data: 0.2739\n",
      "  model: 0.1840\n",
      "  fig: 0.1661\n",
      "  study: 0.1619\n",
      "  using: 0.1601\n",
      "  specie: 0.1578\n",
      "  analysis: 0.1446\n",
      "  figure: 0.1411\n",
      "  cell: 0.1406\n",
      "  doi: 0.1215\n",
      "\n",
      "=== Secondary ラベルの特徴的な単語 (TF-IDF) ===\n",
      "  gene: 0.2423\n",
      "  protein: 0.2389\n",
      "  data: 0.2356\n",
      "  using: 0.1728\n",
      "  genome: 0.1723\n",
      "  cell: 0.1707\n",
      "  alaska: 0.1632\n",
      "  analysis: 0.1624\n",
      "  specie: 0.1614\n",
      "  strain: 0.1495\n",
      "\n",
      "=== Missing ラベルの特徴的な単語 (TF-IDF) ===\n",
      "  gene: 0.3166\n",
      "  data: 0.2115\n",
      "  cell: 0.2068\n",
      "  study: 0.1845\n",
      "  analysis: 0.1770\n",
      "  using: 0.1648\n",
      "  fig: 0.1594\n",
      "  file: 0.1416\n",
      "  additional: 0.1384\n",
      "  model: 0.1319\n"
     ]
    }
   ],
   "source": [
    "# 各ラベルの特徴的な単語を抽出\n",
    "for i, label in enumerate(labels):\n",
    "    # TF-IDFスコアを取得\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    \n",
    "    # スコアでソート\n",
    "    word_scores = list(zip(feature_names, tfidf_scores))\n",
    "    word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n=== {label} ラベルの特徴的な単語 (TF-IDF) ===\")\n",
    "    for word, score in word_scores[:10]:\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDFスコアのみで分類してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分類関数を定義しました（DOIチェック機能付き）\n"
     ]
    }
   ],
   "source": [
    "# 論文分類のための関数を定義（DOIチェック機能付き）\n",
    "def classify_text_by_tfidf(text, tfidf_matrix, labels, feature_names, use_doi_check=True):\n",
    "    # DOI関連パターンを定義\n",
    "    doi_patterns = [\n",
    "        r'10\\.\\d{4,9}/[-._;()/:A-Z0-9]+',  # DOI\n",
    "        r'doi\\.org/[\\w\\./\\-]+',            # DOI URL\n",
    "        r'https?://[\\w\\./\\-]+',            # URL\n",
    "        r'data(?:set|base)?\\s+(?:available|deposited|stored|archived)',  # データセット言及\n",
    "        r'repository\\s+(?:at|available)',  # リポジトリ言及\n",
    "        r'supplementary\\s+(?:data|material|information)',  # 補足データ\n",
    "        r'accession\\s+(?:number|code|id)',  # アクセッション番号\n",
    "        r'github\\.com/[\\w\\./\\-]+',         # GitHub\n",
    "        r'figshare\\.com/[\\w\\./\\-]+',       # Figshare\n",
    "        r'zenodo\\.org/[\\w\\./\\-]+',         # Zenodo\n",
    "        r'dryad\\.org/[\\w\\./\\-]+',          # Dryad\n",
    "        r'genbank',                        # GenBank\n",
    "        r'bioproject',                     # BioProject\n",
    "        r'GSE\\d+|SR[APRX]\\d+|PRJ[NAED][A-Z]?\\d+|E-[A-Z]+-\\d+',  # GEO/SRA/ENA\n",
    "    ]\n",
    "    \n",
    "    # DOIチェック機能が有効な場合\n",
    "    if use_doi_check:\n",
    "        import re\n",
    "        has_doi_match = False\n",
    "        \n",
    "        for pattern in doi_patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                has_doi_match = True\n",
    "                break\n",
    "        \n",
    "        # DOI関連文字列がない場合は確定でMissing判定\n",
    "        if not has_doi_match:\n",
    "            return \"Missing\", 1.0, {\"Primary\": 0.0, \"Secondary\": 0.0, \"Missing\": 1.0}\n",
    "    \n",
    "    # テキストを前処理\n",
    "    processed_words = preprocess_text(text)\n",
    "    \n",
    "    # 各ラベルのスコアを初期化\n",
    "    label_scores = {label: 0.0 for label in labels}\n",
    "    \n",
    "    # 各単語についてTF-IDFスコアを累積\n",
    "    for word in processed_words:\n",
    "        if word in feature_names:\n",
    "            # feature_namesでのインデックスを取得\n",
    "            word_index = np.where(feature_names == word)[0]\n",
    "            if len(word_index) > 0:\n",
    "                word_index = word_index[0]\n",
    "                # 各ラベルのTF-IDFスコアを取得して加算\n",
    "                for i, label in enumerate(labels):\n",
    "                    tfidf_score = tfidf_matrix[i, word_index]\n",
    "                    label_scores[label] += tfidf_score\n",
    "    \n",
    "    # 単語数で正規化\n",
    "    if len(processed_words) > 0:\n",
    "        for label in label_scores:\n",
    "            label_scores[label] /= len(processed_words)\n",
    "    \n",
    "    # 最も高いスコアのラベルを返す\n",
    "    predicted_label = max(label_scores.items(), key=lambda x: x[1])[0]\n",
    "    confidence = max(label_scores.values())\n",
    "    \n",
    "    return predicted_label, confidence, label_scores\n",
    "\n",
    "# 分類性能を評価する関数\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    分類性能を評価\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
    "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"Precision (macro): {precision:.4f}\")\n",
    "    print(f\"Recall (macro): {recall:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  62%|██████▏   | 635/1028 [20:42<06:14,  1.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 1028/1028 [33:52<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7626\n",
      "F1 Score (macro): 0.7596\n",
      "F1 Score (weighted): 0.7675\n",
      "Precision (macro): 0.7698\n",
      "Recall (macro): 0.7768\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Missing       0.73      0.77      0.75       309\n",
      "     Primary       0.62      0.86      0.72       270\n",
      "   Secondary       0.96      0.70      0.81       449\n",
      "\n",
      "    accuracy                           0.76      1028\n",
      "   macro avg       0.77      0.78      0.76      1028\n",
      "weighted avg       0.80      0.76      0.77      1028\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[238  62   9]\n",
      " [ 33 233   4]\n",
      " [ 56  80 313]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tfidf_submission = {'article_id': [], 'predicted_label': [], 'true_label': [], 'label_scores': []}\n",
    "# sample = train_labels.sample(n=100, random_state=42)  # サンプルを取得\n",
    "# for _, row in tqdm(sample.iterrows(), total=sample.shape[0], desc=\"Processing articles\"):\n",
    "for _, row in tqdm(train_labels.iterrows(), total=train_labels.shape[0], desc=\"Processing articles\"):\n",
    "    article_id = row['article_id']\n",
    "    label = row['type']\n",
    "    \n",
    "    # PDFファイルのパスを構築\n",
    "    pdf_path = pdf_dir / f\"{article_id}.pdf\"\n",
    "    \n",
    "    if pdf_path.exists():\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:\n",
    "            predicted_label, confidence, label_scores = classify_text_by_tfidf(text, tfidf_matrix, labels, feature_names)\n",
    "            tfidf_submission['article_id'].append(article_id)\n",
    "            tfidf_submission['predicted_label'].append(predicted_label)\n",
    "            tfidf_submission['true_label'].append(label)\n",
    "            tfidf_submission['label_scores'].append(label_scores)\n",
    "\n",
    "# 性能評価\n",
    "y_true = tfidf_submission['true_label']\n",
    "y_pred = tfidf_submission['predicted_label']\n",
    "evaluate_classification(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>true_label</th>\n",
       "      <th>label_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>Primary</td>\n",
       "      <td>Primary</td>\n",
       "      <td>{'Primary': 0.01830200243619581, 'Secondary': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>{'Primary': 0.014237204037258847, 'Secondary':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>{'Primary': 0.011695045450711728, 'Secondary':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>{'Primary': 0.01470705977951414, 'Secondary': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>{'Primary': 0.010463712002582694, 'Secondary':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id predicted_label true_label  \\\n",
       "0    10.1002_2017jc013030         Primary    Primary   \n",
       "1  10.1002_anie.201916483         Missing    Missing   \n",
       "2  10.1002_anie.202005531         Missing    Missing   \n",
       "3  10.1002_anie.202007717         Missing    Missing   \n",
       "4  10.1002_chem.201902131         Missing    Missing   \n",
       "\n",
       "                                        label_scores  \n",
       "0  {'Primary': 0.01830200243619581, 'Secondary': ...  \n",
       "1  {'Primary': 0.014237204037258847, 'Secondary':...  \n",
       "2  {'Primary': 0.011695045450711728, 'Secondary':...  \n",
       "3  {'Primary': 0.01470705977951414, 'Secondary': ...  \n",
       "4  {'Primary': 0.010463712002582694, 'Secondary':...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_submission_df = pd.DataFrame(tfidf_submission)\n",
    "tfidf_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makedatacount",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
