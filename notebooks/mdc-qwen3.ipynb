{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x77cc95301250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# CRITICAL: Disable vLLM V1 BEFORE importing vllm\n",
    "# V1 does not support logits processors which we need for constrained generation\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import vllm\n",
    "import torch\n",
    "\n",
    "# Import logits processor after setting environment\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "# set seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-28T12:04:44.272511Z",
     "iopub.status.busy": "2025-06-28T12:04:44.272265Z",
     "iopub.status.idle": "2025-06-28T12:04:59.498789Z",
     "shell.execute_reply": "2025-06-28T12:04:59.497891Z",
     "shell.execute_reply.started": "2025-06-28T12:04:44.272487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c46c9fe80741a895ea0357c7121bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "DOI chunks: 3216\n",
      "Accession ID chunks: 1163\n"
     ]
    }
   ],
   "source": [
    "# Configure PDF directory for local development\n",
    "pdf_directory = \"/home/naohiro/MakeDataCount/dataset/train/PDF\"  # Local data directory\n",
    "text_span_len = 1000\n",
    "# Define regex patterns for different identifier types\n",
    "re_doi = re.compile(r\"10\\.\\d{4}\")\n",
    "re_gsr = re.compile(r\"GSE\\d+|SR[APRX]\\d+|PRJ[NAED][A-Z]?\\d+\")\n",
    "re_ipe = re.compile(r\"IPR\\d{6}|PF\\d{5}|EMPIAR-\\d{5}\", re.IGNORECASE)\n",
    "re_c = re.compile(r\"CHEMBL\\d+|CVCL_[A-Z0-9]{4}\")\n",
    "re_e = re.compile(r\"ENS[A-Z]{0,6}[GT]\\d{11}\")\n",
    "re_r = re.compile(r\"N[MC]_\\d+(?:\\.\\d+)?|rs\\d+\")\n",
    "re_u = re.compile(r\"(?:uniprot:)?(?:[OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9][A-Z][A-Z0-9]{2}[0-9])\", re.IGNORECASE)\n",
    "re_g = re.compile(r\"EPI(?:_ISL_)?\\d+\")\n",
    "re_p = re.compile(r\"PXD\\d{6}|SAM[ND]\\d+|ERR\\d+\")\n",
    "relist = [re_gsr, re_ipe, re_c, re_e, re_r, re_g, re_p]\n",
    "\n",
    "def remove_references_section(text):\n",
    "    \"\"\"Remove references section from academic text to focus on main content.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    cut_index = -1\n",
    "    \n",
    "    # Search backwards from end of document starting at 70% mark\n",
    "    for i in range(len(lines) - 1, max(0, int(len(lines) * 0.3)), -1):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Patterns matching common reference section headers\n",
    "        reference_patterns = [\n",
    "            r'^REFERENCES?$',\n",
    "            r'^\\d+\\.?\\s+REFERENCES?$',\n",
    "            r'^\\d+\\.?\\s+References?$',\n",
    "            r'^References?:?$',\n",
    "            r'^BIBLIOGRAPHY$',\n",
    "            r'^\\d+\\.?\\s+BIBLIOGRAPHY$',\n",
    "            r'^\\d+\\.?\\s+Bibliography$',\n",
    "            r'^Bibliography:?$',\n",
    "            r'^Literature\\s+Cited$',\n",
    "            r'^Works\\s+Cited$'\n",
    "        ]\n",
    "        \n",
    "        if any(re.match(pattern, line, re.IGNORECASE) for pattern in reference_patterns):\n",
    "            # Verify following lines contain citation patterns\n",
    "            following_lines = lines[i+1:i+4]\n",
    "            has_citations = False\n",
    "            \n",
    "            for follow_line in following_lines:\n",
    "                if follow_line.strip():\n",
    "                    # Check for citation indicators\n",
    "                    if (re.search(r'\\(\\d{4}\\)', follow_line) or    # Year in parentheses\n",
    "                        re.search(r'\\d{4}\\.', follow_line) or       # Year with period\n",
    "                        'doi:' in follow_line.lower() or           # DOI identifier\n",
    "                        ' et al' in follow_line.lower()):          # Author pattern\n",
    "                        has_citations = True\n",
    "                        break\n",
    "            \n",
    "            # Cut text if citations found or near document end\n",
    "            if has_citations or i >= len(lines) - 3:\n",
    "                cut_index = i\n",
    "                break\n",
    "    \n",
    "    if cut_index != -1:\n",
    "        return '\\n'.join(lines[:cut_index]).strip()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Process PDF files and extract text chunks\n",
    "chunks = []\n",
    "chunks2 = []\n",
    "ids = []\n",
    "if os.path.exists(pdf_directory) and os.listdir(pdf_directory):\n",
    "    for filename in tqdm(os.listdir(pdf_directory), total=len(os.listdir(pdf_directory))):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            \n",
    "            # Extract article identifier from filename\n",
    "            article_id = filename.split(\".pdf\")[0]\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                page_text = page.get_text()\n",
    "                text += page_text + \"\\n\"\n",
    "                \n",
    "            doc.close()\n",
    "\n",
    "            text = remove_references_section(text)\n",
    "\n",
    "            # Extract DOI matches\n",
    "            doi_matches = re_doi.finditer(text)\n",
    "            for match in doi_matches:\n",
    "                if match.group() in article_id: continue\n",
    "                chunk = text[max(0, match.start() - text_span_len): match.start() + text_span_len]\n",
    "                chunks.append((article_id, chunk))\n",
    "\n",
    "            # Extract accession ID matches\n",
    "            for rr in relist:\n",
    "                matches = rr.finditer(text)\n",
    "                for match in matches:\n",
    "                    ids.append(match.group())\n",
    "                    chunk = text[max(0, match.start() - text_span_len): match.start() + text_span_len]\n",
    "                    chunks2.append((article_id, chunk))\n",
    "else:\n",
    "    print(f\"Warning: PDF directory '{pdf_directory}' not found or empty. Please add PDF files to process.\")\n",
    "\n",
    "print(f\"DOI chunks: {len(chunks)}\")\n",
    "print(f\"Accession ID chunks: {len(chunks2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunks[:1000]  # Limit to first 1000 chunks for testing\n",
    "chunks2 = chunks2[:1000]  # Limit to first 1000 chunks for testing\n",
    "ids = ids[:1000]  # Limit to first 1000 IDs for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T12:04:59.499817Z",
     "iopub.status.busy": "2025-06-28T12:04:59.499575Z",
     "iopub.status.idle": "2025-06-28T12:08:19.477676Z",
     "shell.execute_reply": "2025-06-28T12:08:19.47682Z",
     "shell.execute_reply.started": "2025-06-28T12:04:59.499801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-30 19:19:41 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 07-30 19:19:44 [config.py:1604] Using max model len 4096\n",
      "INFO 07-30 19:19:45 [awq_marlin.py:120] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "WARNING 07-30 19:19:45 [config.py:1084] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 07-30 19:19:46 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-30 19:19:46 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/home/naohiro/MakeDataCount/qwen3-8b-awq', speculative_config=None, tokenizer='/home/naohiro/MakeDataCount/qwen3-8b-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/home/naohiro/MakeDataCount/qwen3-8b-awq, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-30 19:19:46 [cuda.py:398] Using Flash Attention backend.\n",
      "INFO 07-30 19:19:47 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-30 19:19:47 [model_runner.py:1083] Starting to load model /home/naohiro/MakeDataCount/qwen3-8b-awq...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d170ca4bea3d476db9e55437faf831f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-30 19:19:48 [default_loader.py:262] Loading weights took 1.29 seconds\n",
      "INFO 07-30 19:19:48 [model_runner.py:1115] Model loading took 5.7071 GiB and 1.372671 seconds\n",
      "INFO 07-30 19:19:50 [worker.py:295] Memory profiling takes 1.26 seconds\n",
      "INFO 07-30 19:19:50 [worker.py:295] the current vLLM instance can use total_gpu_memory (11.72GiB) x gpu_memory_utilization (0.70) = 8.20GiB\n",
      "INFO 07-30 19:19:50 [worker.py:295] model weights take 5.71GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 1.06GiB.\n",
      "INFO 07-30 19:19:50 [executor_base.py:113] # cuda blocks: 484, # CPU blocks: 1820\n",
      "INFO 07-30 19:19:50 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 1.89x\n",
      "INFO 07-30 19:19:51 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 2.79 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use local Qwen3-0.6B model instead of Kaggle input\n",
    "model_path = \"/home/naohiro/MakeDataCount/qwen3-8b-awq\"  # Use HuggingFace model name for local download\n",
    "\n",
    "# Initialize vLLM model with smaller resource requirements for 0.6B model\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    tensor_parallel_size=1,  # Single GPU for small model\n",
    "    gpu_memory_utilization=0.7,  # Lower memory usage for smaller model\n",
    "    trust_remote_code=True,\n",
    "    quantization=\"awq\",  # Use AWQ quantization for smaller model\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=4096,  # Reduced context length for smaller model\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T12:08:19.479033Z",
     "iopub.status.busy": "2025-06-28T12:08:19.478698Z",
     "iopub.status.idle": "2025-06-28T12:08:19.483786Z",
     "shell.execute_reply": "2025-06-28T12:08:19.482938Z",
     "shell.execute_reply.started": "2025-06-28T12:08:19.479005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYS_PROMPT_DOI = \"\"\"\n",
    "You are given a piece of academic text. Your task is to identify a DOI citation that refers specifically to research data.\n",
    "\n",
    "Only respond with either a full normalized DOI URL starting with \"https://doi.org/\" or the word \"Irrelevant\" (without quotes).\n",
    "\n",
    "Do NOT include any other text or explanation.\n",
    "\n",
    "If there is no DOI related to research data, respond with exactly \"Irrelevant\".\n",
    "\n",
    "If multiple DOIs refer to research data, return any one of them.\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT_ACCESSION = \"\"\"\n",
    "You are given a piece of academic text. Your task is to determine whether the provided Accession ID refers to a dataset used in the study.\n",
    "\n",
    "Classify the data associated with the Accession ID as:\n",
    "A) Primary — if the data was generated specifically for this study.\n",
    "B) Secondary — if the data was reused or derived from prior work.\n",
    "C) None — if the ID is mentioned in a different context (e.g., not related to data use, or is unrelated to the study).\n",
    "\n",
    "Respond with only one letter: A, B, or C.\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n",
    "You are given a piece of academic text. Your task is to classify the data associated with the given DOI.\n",
    "\n",
    "Classify the data as:\n",
    "A) Primary: if the data was generated specifically for this study.\n",
    "B) Secondary: if the data was reused or derived from prior work.\n",
    "C) None: if the DOI is part of the References section of a paper, does not refer to research data or is unrelated.\n",
    "\n",
    "Respond with only one letter: A, B, or C.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOI Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T12:08:19.486186Z",
     "iopub.status.busy": "2025-06-28T12:08:19.48586Z",
     "iopub.status.idle": "2025-06-28T12:08:39.655126Z",
     "shell.execute_reply": "2025-06-28T12:08:39.65423Z",
     "shell.execute_reply.started": "2025-06-28T12:08:19.486168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fee35235f9e48129bd6930ad961c1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8130809dad6a4684ae644d18fc305411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-30 20:19:25 [scheduler.py:1822] Sequence group 4747 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    }
   ],
   "source": [
    "prompts = []\n",
    "for article_id, academic_text in chunks:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT_DOI},\n",
    "        {\"role\": \"user\", \"content\": academic_text}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    \n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    vllm.SamplingParams(\n",
    "        seed=0,\n",
    "        skip_special_tokens=True,\n",
    "        max_tokens=80,\n",
    "        temperature=0\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "responses = [output.outputs[0].text.strip() for output in outputs]\n",
    "\n",
    "doi_pattern = re.compile(r'(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)', re.I)\n",
    "\n",
    "doi_urls = []\n",
    "for response in responses:\n",
    "    if response.lower() == \"irrelevant\":\n",
    "        doi_urls.append(\"Irrelevant\")\n",
    "    else:\n",
    "        match = doi_pattern.search(response)\n",
    "        if match:\n",
    "            doi_urls.append(\"https://doi.org/\" + match.group(1))\n",
    "        else:\n",
    "            doi_urls.append(\"Irrelevant\")  # fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOI Classification\n",
    "\n",
    "Constrained generation using MultipleChoiceLogitsProcessor to ensure valid class selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T12:08:39.656045Z",
     "iopub.status.busy": "2025-06-28T12:08:39.655814Z",
     "iopub.status.idle": "2025-06-28T12:08:48.889899Z",
     "shell.execute_reply": "2025-06-28T12:08:48.88904Z",
     "shell.execute_reply.started": "2025-06-28T12:08:39.656027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9b3311a1a242a88cba3364086b294d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9528d97fb2442f596b99c740c59fc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = []\n",
    "valid_indices = []\n",
    "for i, (chunk, url) in enumerate(zip(chunks, doi_urls)):\n",
    "    #if url == \"Irrelevant\":\n",
    "    #    continue  # Skip irrelevant DOIs\n",
    "\n",
    "    article_id, academic_text = chunk\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT_CLASSIFY_DOI},\n",
    "        {\"role\": \"user\", \"content\": f\"DOI: {url}\\n\\nAcademic text:\\n{academic_text}\"}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "    valid_indices.append(i)\n",
    "\n",
    "# Initialize logits processor for constrained generation\n",
    "mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\", \"C\"])\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    vllm.SamplingParams(\n",
    "        temperature=0.1,\n",
    "        skip_special_tokens=True,\n",
    "        max_tokens=1,\n",
    "        logits_processors=[mclp],\n",
    "        logprobs=len(mclp.choices)\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "# Extract log probabilities for each choice\n",
    "logprobs = []\n",
    "for lps in [output.outputs[0].logprobs[0].values() for output in outputs]:\n",
    "    logprobs.append({lp.decoded_token: lp.logprob for lp in list(lps)})\n",
    "\n",
    "logit_matrix = pd.DataFrame(logprobs)[[\"A\", \"B\", \"C\"]].values\n",
    "choices = [\"Primary\", \"Secondary\", None]\n",
    "answers = [choices[pick] for pick in np.argmax(logit_matrix, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T12:08:48.890925Z",
     "iopub.status.busy": "2025-06-28T12:08:48.89065Z",
     "iopub.status.idle": "2025-06-28T12:08:55.285958Z",
     "shell.execute_reply": "2025-06-28T12:08:55.285117Z",
     "shell.execute_reply.started": "2025-06-28T12:08:48.890896Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3e006658284e43890c8c52cae9fe6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7aff22834b447da15c81242cc0976b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process accession IDs for classification\n",
    "prompts = []\n",
    "for chunk, acc_id in zip(chunks2, ids):\n",
    "    article_id, academic_text = chunk\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT_ACCESSION},\n",
    "        {\"role\": \"user\", \"content\": f\"Accession ID: {acc_id}\\n\\nAcademic text:\\n{academic_text}\"}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    vllm.SamplingParams(\n",
    "        temperature=0.1,\n",
    "        skip_special_tokens=True,\n",
    "        max_tokens=1,\n",
    "        logits_processors=[mclp],\n",
    "        logprobs=len(mclp.choices)\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "# Extract log probabilities for accession ID classifications\n",
    "logprobs2 = []\n",
    "for lps in [output.outputs[0].logprobs[0].values() for output in outputs]:\n",
    "    logprobs2.append({lp.decoded_token: lp.logprob for lp in list(lps)})\n",
    "\n",
    "logit_matrix2 = pd.DataFrame(logprobs2)[[\"A\", \"B\", \"C\"]].values\n",
    "choices2 = [\"Primary\", \"Secondary\", None]\n",
    "answers2 = [choices2[pick] for pick in np.argmax(logit_matrix2, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T12:08:55.287737Z",
     "iopub.status.busy": "2025-06-28T12:08:55.286926Z",
     "iopub.status.idle": "2025-06-28T12:08:55.32831Z",
     "shell.execute_reply": "2025-06-28T12:08:55.327653Z",
     "shell.execute_reply.started": "2025-06-28T12:08:55.287706Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "Primary    713\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create submission dataframes\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"article_id\"] = [c[0] for c in chunks]\n",
    "sub_df[\"dataset_id\"] = doi_urls\n",
    "sub_df[\"dataset_id\"] = sub_df[\"dataset_id\"].str.lower()\n",
    "sub_df[\"type\"] = answers\n",
    "sub_df = sub_df[sub_df[\"type\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "sub_df2 = pd.DataFrame()\n",
    "sub_df2[\"article_id\"] = [c[0] for c in chunks2]\n",
    "sub_df2[\"dataset_id\"] = ids\n",
    "sub_df2[\"type\"] = answers2\n",
    "sub_df2 = sub_df2[sub_df2[\"type\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "# Combine and deduplicate results\n",
    "sub_df = pd.concat([sub_df, sub_df2], ignore_index=True)\n",
    "sub_df = sub_df[sub_df[\"type\"].isin([\"Primary\", \"Secondary\"])].reset_index(drop=True)\n",
    "sub_df = sub_df.sort_values(by=[\"article_id\", \"dataset_id\", \"type\"], ascending=False)\\\n",
    "               .drop_duplicates(subset=['article_id', 'dataset_id'], keep=\"first\")\\\n",
    "               .reset_index(drop=True)\n",
    "\n",
    "# Generate submission file\n",
    "sub_df['row_id'] = range(len(sub_df))\n",
    "print(sub_df[\"type\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 50\n",
      "FP: 663\n",
      "FN: 669\n",
      "F1 Score: 0.07\n"
     ]
    }
   ],
   "source": [
    "def f1_score(tp, fp, fn):\n",
    "    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    \n",
    "label_df = pd.read_csv(\"../dataset/train_labels.csv\")\n",
    "label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "\n",
    "hits_df = label_df.merge(sub_df, on=[\"article_id\", \"dataset_id\", \"type\"])\n",
    "\n",
    "tp = hits_df.shape[0]\n",
    "fp = sub_df.shape[0] - tp\n",
    "fn = label_df.shape[0] - tp\n",
    "\n",
    "print(\"TP:\", tp)\n",
    "print(\"FP:\", fp)\n",
    "print(\"FN:\", fn)\n",
    "print(\"F1 Score:\", round(f1_score(tp, fp, fn), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13015230,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "modelId": 161088,
     "modelInstanceId": 138579,
     "sourceId": 162952,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141565,
     "sourceId": 166368,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 322000,
     "modelInstanceId": 310551,
     "sourceId": 375840,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "makedatacount",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
