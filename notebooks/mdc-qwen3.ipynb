{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"sourceType":"competition"},{"sourceId":162952,"sourceType":"modelInstanceVersion","modelInstanceId":138579,"modelId":161088},{"sourceId":166368,"sourceType":"modelInstanceVersion","modelInstanceId":141565,"modelId":164048},{"sourceId":375840,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":310551,"modelId":322000}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Read Data and Extract DOI Links","metadata":{}},{"cell_type":"code","source":"import os\n\n# vLLM V1 does not currently accept logits processor so we need to disable it\n# https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html#deprecated-features\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nimport re\nimport fitz  # PyMuPDF\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nimport pickle\nimport vllm\nimport torch\n\n# Step 1: Read all PDFs and convert to text\n# pdf_directory = \"/kaggle/input/make-data-count-finding-data-references/test/PDF\"\n\npdf_directory = \"/kaggle/input/make-data-count-finding-data-references/test/PDF\" \\\n                if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n                else \"/kaggle/input/make-data-count-finding-data-references/train/PDF\"\n\nchunks = []\nchunks2 = []\ntext_span_len = 500\n\nre_doi = re.compile(r\"10\\.\\d{4}\")\nre_gsr = re.compile(r\"GSE\\d+|SR[APRX]\\d+|PRJ[NAED][A-Z]?\\d+\")\nre_ipe = re.compile(r\"IPR\\d{6}|PF\\d{5}|EMPIAR-\\d{5}\", re.IGNORECASE)\nre_c = re.compile(r\"CHEMBL\\d+|CVCL_[A-Z0-9]{4}\")\nre_e = re.compile(r\"ENS[A-Z]{0,6}[GT]\\d{11}\")\nre_r = re.compile(r\"N[MC]_\\d+(?:\\.\\d+)?|rs\\d+\")\nre_u = re.compile(r\"(?:uniprot:)?(?:[OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9][A-Z][A-Z0-9]{2}[0-9])\", re.IGNORECASE)\nre_g = re.compile(r\"EPI(?:_ISL_)?\\d+\")\nre_p = re.compile(r\"PXD\\d{6}|SAM[ND]\\d+|ERR\\d+\")\n\nrelist = [re_gsr, re_ipe, re_c, re_e, re_r, re_g, re_p]\n\nids = []\n\ndef remove_references_section(text):\n    lines = text.split('\\n')\n    cut_index = -1\n    \n    # Look backwards from end of document\n    for i in range(len(lines) - 1, max(0, int(len(lines) * 0.3)), -1):\n        line = lines[i].strip()\n        \n        obvious_patterns = [\n            r'^REFERENCES?$',\n            r'^\\d+\\.?\\s+REFERENCES?$',\n            r'^\\d+\\.?\\s+References?$',\n            r'^References?:?$',\n            r'^BIBLIOGRAPHY$',\n            r'^\\d+\\.?\\s+BIBLIOGRAPHY$',\n            r'^\\d+\\.?\\s+Bibliography$',\n            r'^Bibliography:?$',\n            r'^Literature\\s+Cited$',\n            r'^Works\\s+Cited$'\n        ]\n        \n        if any(re.match(pattern, line, re.IGNORECASE) for pattern in obvious_patterns):\n            # Double-check: look at following lines for citation patterns\n            following_lines = lines[i+1:i+4]\n            has_citations = False\n            \n            for follow_line in following_lines:\n                if follow_line.strip():\n                    # Check for obvious citation patterns\n                    if (re.search(r'\\(\\d{4}\\)', follow_line) or    # (2020)\n                        re.search(r'\\d{4}\\.', follow_line) or       # 2020.\n                        'doi:' in follow_line.lower() or           # DOI\n                        ' et al' in follow_line.lower()):          # et al\n                        has_citations = True\n                        break\n            \n            # Only cut if we found citation-like content\n            if has_citations or i >= len(lines) - 3:  # Or very near end\n                cut_index = i\n                break\n    \n    if cut_index != -1:\n        return '\\n'.join(lines[:cut_index]).strip()\n    \n    return text.strip()\n\nfor filename in tqdm(os.listdir(pdf_directory), total=len(os.listdir(pdf_directory))):\n    if filename.endswith(\".pdf\"):\n        pdf_path = os.path.join(pdf_directory, filename)\n        \n        # Extract article_id from filename\n        article_id = filename.split(\".pdf\")[0]\n        doc = fitz.open(pdf_path)\n        text = \"\"\n        for page in doc:\n            page_text = page.get_text()\n            text += page_text + \"\\n\"\n            \n        doc.close()\n\n        text = remove_references_section(text)\n\n        doi_matches = re_doi.finditer(text)\n        for match in doi_matches:\n            if match.group() in article_id: continue\n            chunk = text[max(0, match.start() - text_span_len): match.start() + text_span_len]\n            chunks.append((article_id, chunk))\n\n        for rr in relist:\n            matches = rr.finditer(text)\n            for match in matches:\n                ids.append(match.group())\n                chunk = text[max(0, match.start() - text_span_len): match.start() + text_span_len]\n                chunks2.append((article_id, chunk))\nprint(len(chunks))\nprint(len(chunks2))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-28T12:04:44.272265Z","iopub.execute_input":"2025-06-28T12:04:44.272511Z","iopub.status.idle":"2025-06-28T12:04:59.498789Z","shell.execute_reply.started":"2025-06-28T12:04:44.272487Z","shell.execute_reply":"2025-06-28T12:04:59.497891Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load LLM","metadata":{}},{"cell_type":"code","source":"model_path = \"/kaggle/input/qwen3/transformers/32b-awq/1\"\n\nllm = vllm.LLM(\n    model_path,\n    quantization='awq',\n    tensor_parallel_size=torch.cuda.device_count(),\n    gpu_memory_utilization=0.91,\n    trust_remote_code=True,\n    dtype=\"half\",\n    enforce_eager=True,\n    max_model_len=5120,\n    disable_log_stats=True,\n    enable_prefix_caching=True\n)\ntokenizer = llm.get_tokenizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T12:04:59.499575Z","iopub.execute_input":"2025-06-28T12:04:59.499817Z","iopub.status.idle":"2025-06-28T12:08:19.477676Z","shell.execute_reply.started":"2025-06-28T12:04:59.499801Z","shell.execute_reply":"2025-06-28T12:08:19.47682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# System prompts","metadata":{}},{"cell_type":"code","source":"SYS_PROMPT_DOI = \"\"\"\nYou are given a piece of academic text. Your task is to identify a DOI citation that refers specifically to research data.\n\nOnly respond with either a full normalized DOI URL starting with \"https://doi.org/\" or the word \"Irrelevant\" (without quotes).\n\nDo NOT include any other text or explanation.\n\nIf there is no DOI related to research data, respond with exactly \"Irrelevant\".\n\nIf multiple DOIs refer to research data, return any one of them.\n\"\"\"\n\nSYS_PROMPT_ACCESSION = \"\"\"\nYou are given a piece of academic text. Your task is to determine whether the provided Accession ID refers to a dataset used in the study.\n\nClassify the data associated with the Accession ID as:\nA) Primary — if the data was generated specifically for this study.\nB) Secondary — if the data was reused or derived from prior work.\nC) None — if the ID is mentioned in a different context (e.g., not related to data use, or is unrelated to the study).\n\nRespond with only one letter: A, B, or C.\n\"\"\"\n\nSYS_PROMPT_CLASSIFY_DOI = \"\"\"\nYou are given a piece of academic text. Your task is to classify the data associated with the given DOI.\n\nClassify the data as:\nA) Primary: if the data was generated specifically for this study.\nB) Secondary: if the data was reused or derived from prior work.\nC) None: if the DOI is part of the References section of a paper, does not refer to research data or is unrelated.\n\nRespond with only one letter: A, B, or C.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T12:08:19.478698Z","iopub.execute_input":"2025-06-28T12:08:19.479033Z","iopub.status.idle":"2025-06-28T12:08:19.483786Z","shell.execute_reply.started":"2025-06-28T12:08:19.479005Z","shell.execute_reply":"2025-06-28T12:08:19.482938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ask LLM to extract DOI links","metadata":{}},{"cell_type":"code","source":"prompts = []\nfor article_id, academic_text in chunks:\n    messages = [\n        {\"role\": \"system\", \"content\": SYS_PROMPT_DOI},\n        {\"role\": \"user\", \"content\": academic_text}\n    ]\n\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=False,\n    )\n    \n    prompts.append(prompt)\n\noutputs = llm.generate(\n    prompts,\n    vllm.SamplingParams(\n        seed=0,\n        skip_special_tokens=True,\n        max_tokens=80,\n        temperature=0\n    ),\n    use_tqdm=True\n)\n\nresponses = [output.outputs[0].text.strip() for output in outputs]\n\ndoi_pattern = re.compile(r'(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)', re.I)\n\ndoi_urls = []\nfor response in responses:\n    if response.lower() == \"irrelevant\":\n        doi_urls.append(\"Irrelevant\")\n    else:\n        match = doi_pattern.search(response)\n        if match:\n            doi_urls.append(\"https://doi.org/\" + match.group(1))\n        else:\n            doi_urls.append(\"Irrelevant\")  # fallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T12:08:19.48586Z","iopub.execute_input":"2025-06-28T12:08:19.486186Z","iopub.status.idle":"2025-06-28T12:08:39.655126Z","shell.execute_reply.started":"2025-06-28T12:08:19.486168Z","shell.execute_reply":"2025-06-28T12:08:39.65423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ask LLM to classify DOI links\nUse logits-processor-zoo MultipleChoiceLogitsProcessor to enforce LLM choose between classes.","metadata":{}},{"cell_type":"code","source":"prompts = []\nvalid_indices = []\nfor i, (chunk, url) in enumerate(zip(chunks, doi_urls)):\n    if url == \"Irrelevant\":\n        continue  # skip irrelevant\n\n    article_id, academic_text = chunk\n    messages = [\n        {\"role\": \"system\", \"content\": SYS_PROMPT_CLASSIFY_DOI},\n        {\"role\": \"user\", \"content\": f\"DOI: {url}\\n\\nAcademic text:\\n{academic_text}\"}\n    ]\n\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=False,\n    )\n    prompts.append(prompt)\n    valid_indices.append(i)\n\nmclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\", \"C\"])\n\noutputs = llm.generate(\n    prompts,\n    vllm.SamplingParams(\n        seed=777,\n        temperature=0.1,\n        skip_special_tokens=True,\n        max_tokens=1,\n        logits_processors=[mclp],\n        logprobs=len(mclp.choices)\n    ),\n    use_tqdm=True\n)\n\nlogprobs = []\nfor lps in [output.outputs[0].logprobs[0].values() for output in outputs]:\n    logprobs.append({lp.decoded_token: lp.logprob for lp in list(lps)})\n\nlogit_matrix = pd.DataFrame(logprobs)[[\"A\", \"B\", \"C\"]].values\nchoices = [\"Primary\", \"Secondary\", None]\nanswers = [None] * len(chunks)\n\nfor i, pick in zip(valid_indices, np.argmax(logit_matrix, axis=1)):\n    answers[i] = choices[pick]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T12:08:39.655814Z","iopub.execute_input":"2025-06-28T12:08:39.656045Z","iopub.status.idle":"2025-06-28T12:08:48.889899Z","shell.execute_reply.started":"2025-06-28T12:08:39.656027Z","shell.execute_reply":"2025-06-28T12:08:48.88904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompts = []\nfor chunk, acc_id in zip(chunks2, ids):\n    article_id, academic_text = chunk\n    messages = [\n        {\"role\": \"system\", \"content\": SYS_PROMPT_ACCESSION},\n        {\"role\": \"user\", \"content\": f\"Accession ID: {acc_id}\\n\\nAcademic text:\\n{academic_text}\"}\n    ]\n\n    prompt = tokenizer.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=False,\n    )\n    prompts.append(prompt)\n\noutputs = llm.generate(\n    prompts,\n    vllm.SamplingParams(\n        seed=777,\n        temperature=0.1,\n        skip_special_tokens=True,\n        max_tokens=1,\n        logits_processors=[mclp],\n        logprobs=len(mclp.choices)\n    ),\n    use_tqdm=True\n)\n\nlogprobs2 = []\nfor lps in [output.outputs[0].logprobs[0].values() for output in outputs]:\n    logprobs2.append({lp.decoded_token: lp.logprob for lp in list(lps)})\n\nlogit_matrix2 = pd.DataFrame(logprobs2)[[\"A\", \"B\", \"C\"]].values\nchoices2 = [\"Primary\", \"Secondary\", None]\nanswers2 = [choices2[pick] for pick in np.argmax(logit_matrix2, axis=1)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T12:08:48.89065Z","iopub.execute_input":"2025-06-28T12:08:48.890925Z","iopub.status.idle":"2025-06-28T12:08:55.285958Z","shell.execute_reply.started":"2025-06-28T12:08:48.890896Z","shell.execute_reply":"2025-06-28T12:08:55.285117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare Submission","metadata":{}},{"cell_type":"code","source":"sub_df = pd.DataFrame()\nsub_df[\"article_id\"] = [c[0] for c in chunks]\nsub_df[\"dataset_id\"] = doi_urls\nsub_df[\"dataset_id\"] = sub_df[\"dataset_id\"].str.lower()\nsub_df[\"type\"] = answers\nsub_df = sub_df[sub_df[\"type\"].notnull()].reset_index(drop=True)\n\nsub_df2 = pd.DataFrame()\nsub_df2[\"article_id\"] = [c[0] for c in chunks2]\nsub_df2[\"dataset_id\"] = ids\nsub_df2[\"type\"] = answers2\nsub_df2 = sub_df2[sub_df2[\"type\"].notnull()].reset_index(drop=True)\n\nsub_df = pd.concat([sub_df, sub_df2], ignore_index=True)\nsub_df = sub_df[sub_df[\"type\"].isin([\"Primary\", \"Secondary\"])].reset_index(drop=True)\nsub_df = sub_df.sort_values(by=[\"article_id\", \"dataset_id\", \"type\"], ascending=False)\\\n               .drop_duplicates(subset=['article_id', 'dataset_id'], keep=\"first\")\\\n               .reset_index(drop=True)\n\nsub_df['row_id'] = range(len(sub_df))\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"article_id\", \"dataset_id\", \"type\"])\n\nprint(sub_df[\"type\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T12:08:55.286926Z","iopub.execute_input":"2025-06-28T12:08:55.287737Z","iopub.status.idle":"2025-06-28T12:08:55.32831Z","shell.execute_reply.started":"2025-06-28T12:08:55.287706Z","shell.execute_reply":"2025-06-28T12:08:55.327653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate validation score","metadata":{}},{"cell_type":"code","source":"def f1_score(tp, fp, fn):\n    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n    \n    \nif not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    pred_df = pd.read_csv(\"submission.csv\")\n    label_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n    label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n\n    hits_df = label_df.merge(pred_df, on=[\"article_id\", \"dataset_id\", \"type\"])\n    \n    tp = hits_df.shape[0]\n    fp = pred_df.shape[0] - tp\n    fn = label_df.shape[0] - tp\n    \n    print(\"TP:\", tp)\n    print(\"FP:\", fp)\n    print(\"FN:\", fn)\n    print(\"F1 Score:\", round(f1_score(tp, fp, fn), 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T12:08:55.329048Z","iopub.execute_input":"2025-06-28T12:08:55.329358Z","iopub.status.idle":"2025-06-28T12:08:55.361431Z","shell.execute_reply.started":"2025-06-28T12:08:55.329338Z","shell.execute_reply":"2025-06-28T12:08:55.360847Z"}},"outputs":[],"execution_count":null}]}